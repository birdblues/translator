from .protector import MarkdownProtector
from .chunker import AdaptiveMarkdownChunker
import requests
from typing import Optional, List


class MarkdownTranslator:
    """
    ÎßàÌÅ¨Îã§Ïö¥ Î¨∏ÏÑúÎ•º ÌïúÍµ≠Ïñ¥Î°ú Î≤àÏó≠ÌïòÎäî ÌÅ¥ÎûòÏä§
    - ÎßàÌÅ¨Îã§Ïö¥ Íµ¨Ï°∞ÏôÄ Ìè¨Îß∑ÌåÖÏùÑ Î≥¥Ï°¥ÌïòÎ©¥ÏÑú Î≤àÏó≠
    - Ï≤≠ÌÇπÏùÑ ÌÜµÌïú ÎåÄÏö©Îüâ Î¨∏ÏÑú Ï≤òÎ¶¨
    - Ollama APIÎ•º ÌÜµÌïú Î°úÏª¨ LLM ÌôúÏö©
    """
    
    DEFAULT_SYSTEM_PROMPT = """You are a professional Korean translator specializing in technical documentation. Your task is to translate markdown content while preserving all formatting and maintaining consistent tone and style.

TRANSLATION TONE & STYLE:
- Use consistent formal polite tone (Ìï©ÎãàÎã§/ÏäµÎãàÎã§ Ï≤¥) throughout
- Maintain professional but accessible language suitable for technical tutorials
- Use natural Korean expressions, avoiding direct word-by-word translation
- Keep the same level of formality as Korean technical documentation

FORMATTING RULES:
1. Keep ALL markdown formatting symbols (*, **, #, ##, ###, -, `, ```, etc.) exactly as they are
2. DO NOT translate content inside code blocks (```...``` or `...`)
3. DO NOT translate code comments or inline code (e.g., `code()`)
4. DO NOT translate URLs, file paths, or technical identifiers
5. DO NOT translate programming language names, function names, or variable names
6. Keep the exact same structure and formatting
7. Only translate natural language text content
8. Preserve line breaks and spacing exactly
9. Always preserve inline math formulas in $...$ format and math blocks in $$...$$ format.
10. DO NOT translate publication notation lines with (...)= format.
11. DO NOT translate "__YAML_FRONT_MATTER__"
12. DO NOT translate "__CODE_BLOCK_{i}__"
13. DO NOT translate "__INDENT_BLOCK_{i}__"
14. DO NOT translate "__MATH_BLOCK_{i}__"
15. DO NOT translate "__TABLE_BLOCK_{i}__"
16. DO NOT translate "__HTML_TAG_{i}__"
17. DO NOT translate "__OBSIDIAN_LINK_{i}__
18. DO NOT delete any placeholders(__YAML_FRONT_MATTER__, __CODE_BLOCK_{i}__, ...) or special markers
19. DO NOT translate file names, paths, or identifiers in any form (e.g., `file.txt`, `path/to/file`, `variable_name`)
20. Ï§ÑÎ∞îÍøàÍ≥º Í≥µÎ∞±ÏùÄ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄÌï©ÎãàÎã§.

CONSISTENCY RULES:
- Maintain consistent terminology throughout the document
- Use the same Korean terms for repeated English technical terms
- Follow the translation style established in previous sections

TONE EXAMPLES:
- "Let's check this out" ‚Üí "Ïù¥Î•º ÌôïÏù∏Ìï¥Î≥¥Í≤†ÏäµÎãàÎã§" (not "ÌôïÏù∏Ìï¥ Î≥¥Ï£†")
- "Great!" ‚Üí "ÌõåÎ•≠Ìï©ÎãàÎã§!" (not "Ï¢ãÏïÑÏöî!")
- "You can see..." ‚Üí "Îã§ÏùåÍ≥º Í∞ôÏù¥ ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§" (not "Î≥º Ïàò ÏûàÏñ¥Ïöî")"""

    def __init__(
        self,
        model: str = "qwen3:32b",
        api_url: str = "http://localhost:11434/api/generate",
        max_tokens: int = 1024,
        temperature: float = 0.1,
        system_prompt: Optional[str] = None,
        verbose: bool = True
    ):
        """
        Î≤àÏó≠Í∏∞ Ï¥àÍ∏∞Ìôî
        
        Args:
            model: ÏÇ¨Ïö©Ìï† LLM Î™®Îç∏Î™Ö
            api_url: Ollama API URL
            max_tokens: Ï≤≠ÌÅ¨Îãπ ÏµúÎåÄ ÌÜ†ÌÅ∞ Ïàò
            temperature: Î≤àÏó≠ Ï∞ΩÏùòÏÑ± ÏàòÏ§Ä (0.0-1.0)
            system_prompt: Ïª§Ïä§ÌÖÄ ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ (NoneÏù¥Î©¥ Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©)
            verbose: ÏßÑÌñâ ÏÉÅÌô© Ï∂úÎ†• Ïó¨Î∂Ä
        """
        self.model = model
        self.api_url = api_url
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.system_prompt = system_prompt or self.DEFAULT_SYSTEM_PROMPT
        self.verbose = verbose
        
        # ÏùòÏ°¥ÏÑ± Í∞ùÏ≤¥Îì§ Ï¥àÍ∏∞Ìôî
        self.protector = MarkdownProtector()
        self.chunker = AdaptiveMarkdownChunker(max_tokens=max_tokens)
    
    def translate(self, text: str) -> str:
        """
        ÎßàÌÅ¨Îã§Ïö¥ ÌÖçÏä§Ìä∏Î•º ÌïúÍµ≠Ïñ¥Î°ú Î≤àÏó≠
        
        Args:
            text: Î≤àÏó≠Ìï† ÎßàÌÅ¨Îã§Ïö¥ ÌÖçÏä§Ìä∏
            
        Returns:
            Î≤àÏó≠Îêú ÎßàÌÅ¨Îã§Ïö¥ ÌÖçÏä§Ìä∏
        """
        if self.verbose:
            print("ÎßàÌÅ¨Îã§Ïö¥ Î≥¥Ìò∏ ÏãúÏûë...")
        
        # 1. ÎßàÌÅ¨Îã§Ïö¥ Íµ¨Ï°∞ Î≥¥Ìò∏
        protected_text = self.protector.protect(text)
        
        # print(f"[START PROTECTOR]\n{protected_text}\n[END PROTECTOR]")

        if self.verbose:
            print(f"ÌÖçÏä§Ìä∏ Ï≤≠ÌÇπ ÏãúÏûë... {self.max_tokens} ÌÜ†ÌÅ∞ Í∏∞Ï§Ä")
        
        # 2. ÌÖçÏä§Ìä∏ Ï≤≠ÌÇπ
        chunks = self.chunker.split_text(protected_text)
        
        # print(chunks)
        
        if self.verbose:
            print(f"Ï¥ù {len(chunks)}Í∞ú Ï≤≠ÌÅ¨Î°ú Î∂ÑÌï†Îê®")
        
        # 3. Í∞Å Ï≤≠ÌÅ¨ Î≤àÏó≠
        translated_chunks = []
        for i, chunk in enumerate(chunks):
            token_count = chunk.metadata.get('token_count', 0)
            
            if self.verbose:
                print(f"Ï≤≠ÌÅ¨ {i+1}/{len(chunks)} Ï≤òÎ¶¨ Ï§ë... ({token_count} ÌÜ†ÌÅ∞)")

            # print("Ï≤≠ÌÅ¨ ÎÇ¥Ïö©:") 
            # print(chunk.page_content)
            translated = self._generate_text(chunk.page_content)
            # print("Î≤àÏó≠ Í≤∞Í≥º:")
            # print(translated)
            translated_chunks.append(translated)
        
        # 4. Î≤àÏó≠Îêú Ï≤≠ÌÅ¨Îì§ Í≤∞Ìï©
        translated_text = "\n".join(translated_chunks)
        
        # print(translated_text)
        
        if self.verbose:
            print("ÎßàÌÅ¨Îã§Ïö¥ Íµ¨Ï°∞ Î≥µÏõê Ï§ë...")
        
        # 5. ÎßàÌÅ¨Îã§Ïö¥ Íµ¨Ï°∞ Î≥µÏõê
        result = self.protector.restore(translated_text)
        
        # print(result)
        
        if self.verbose:
            print("Î≤àÏó≠ ÏôÑÎ£å!")
        
        return result
    
    def _generate_text(self, prompt: str) -> str:
        """
        LLMÏùÑ ÏÇ¨Ïö©Ìïú ÌÖçÏä§Ìä∏ ÏÉùÏÑ±
        
        Args:
            prompt: Î≤àÏó≠Ìï† ÌÖçÏä§Ìä∏
            
        Returns:
            Î≤àÏó≠Îêú ÌÖçÏä§Ìä∏
        """
        data = {
            "model": self.model,
            "temperature": self.temperature,
            "think": False,  # ÏÉùÍ∞Å Î™®Îìú ÎπÑÌôúÏÑ±Ìôî
            "system": self.system_prompt,
            "prompt": prompt,
            "stream": False  # Ïä§Ìä∏Î¶¨Î∞ç ÎπÑÌôúÏÑ±Ìôî
        }
        
        try:
            response = requests.post(self.api_url, json=data)
            
            if response.status_code == 200:
                result = response.json()
                return result['response']
            else:
                error_msg = f"API Error: {response.status_code}"
                if self.verbose:
                    print(error_msg)
                return f"Error: {response.status_code}"
                
        except requests.RequestException as e:
            error_msg = f"Request Error: {str(e)}"
            if self.verbose:
                print(error_msg)
            return f"Error: {str(e)}"
    
    def translate_file(self, input_path: str, output_path: str) -> bool:
        """
        ÌååÏùºÏùÑ ÏùΩÏñ¥ÏÑú Î≤àÏó≠ ÌõÑ Ï†ÄÏû•
        
        Args:
            input_path: ÏûÖÎ†• ÌååÏùº Í≤ΩÎ°ú
            output_path: Ï∂úÎ†• ÌååÏùº Í≤ΩÎ°ú
            
        Returns:
            ÏÑ±Í≥µ Ïó¨Î∂Ä
        """
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            translated = self.translate(content)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(translated)
            
            if self.verbose:
                print(f"Î≤àÏó≠ ÏôÑÎ£å: {input_path} ‚Üí {output_path}")
            
            return True
            
        except Exception as e:
            if self.verbose:
                print(f"ÌååÏùº Ï≤òÎ¶¨ Ïò§Î•ò: {str(e)}")
            return False
    
    def get_translation_stats(self, text: str) -> dict:
        """
        Î≤àÏó≠ ÏûëÏóÖÏóê ÎåÄÌïú ÌÜµÍ≥Ñ Ï†ïÎ≥¥ Î∞òÌôò
        
        Args:
            text: Î∂ÑÏÑùÌï† ÌÖçÏä§Ìä∏
            
        Returns:
            ÌÜµÍ≥Ñ Ï†ïÎ≥¥ ÎîïÏÖîÎÑàÎ¶¨
        """
        protected_text = self.protector.protect(text)
        chunks = self.chunker.split_text(protected_text)
        stats = self.chunker.get_chunk_stats(chunks)
        
        return {
            "original_length": len(text),
            "protected_length": len(protected_text),
            "total_chunks": stats["total_chunks"],
            "total_tokens": stats["total_tokens"],
            "avg_tokens_per_chunk": stats["avg_tokens_per_chunk"],
            "estimated_api_calls": len(chunks)
        }


# ÏÇ¨Ïö© ÏòàÏãú
if __name__ == "__main__":
    # Î≤àÏó≠Í∏∞ ÏÉùÏÑ±
    translator = MarkdownTranslator(
        model="qwen3:32b",
        max_tokens=1024, # Î™®Îç∏Ïóê Îî∞Îùº Ï°∞Ï†ï ÌïÑÏöî 32b Î™®Îç∏ÏùÄ ÌòÑÏû¨ ÎπÑÎîîÏò§ Î©îÎ™®Î¶¨ Ï†úÌïúÏúºÎ°ú contextÍ∞Ä 4096 ÌÜ†ÌÅ∞ÍπåÏßÄ Í∞ÄÎä•ÌïòÎØÄÎ°ú ÌïúÍ∏Ä ÌäπÏÑ±ÏÉÅ 1/4Î°ú ÏÑ§Ï†ï 
        temperature=0.1,
        verbose=True
    )
    
    # ÌÖçÏä§Ìä∏ Î≤àÏó≠
    markdown_text = r"""
---
created: 2025-06-17T17:22:07 (UTC +09:00)
tags: []
source: https://docling-project.github.io/docling/usage/
author: 
---

# Usage - Docling

> ## Excerpt
> To convert individual PDF documents, use convert(), for example:

---
## Conversion

### Convert a single document

To convert individual PDF documents, use `convert()`, for example:

```python
from docling.document_converter import DocumentConverter

source = "https://arxiv.org/pdf/2408.09869"  # PDF path or URL
converter = DocumentConverter()
result = converter.convert(source)
print(result.document.export_to_markdown())  # output: "### Docling Technical Report[...]"
```

### CLI

You can also use Docling directly from your command line to convert individual files ‚Äîbe it local or by URL‚Äî or whole directories.

```
docling https://arxiv.org/pdf/2206.01062
```

You can also use ü•ö[SmolDocling](https://huggingface.co/ds4sd/SmolDocling-256M-preview) and other VLMs via Docling CLI:

```
docling --pipeline vlm --vlm-model smoldocling https://arxiv.org/pdf/2206.01062
```

This will use MLX acceleration on supported Apple Silicon hardware.

To see all available options (export formats etc.) run `docling --help`. More details in the [CLI reference page](https://docling-project.github.io/docling/reference/cli/).

### Advanced options

#### Model prefetching and offline usage

By default, models are downloaded automatically upon first usage. If you would prefer to explicitly prefetch them for offline use (e.g. in air-gapped environments) you can do that as follows:

**Step 1: Prefetch the models**

Use the `docling-tools models download` utility:

```
$ docling-tools models download
Downloading layout model...
Downloading tableformer model...
Downloading picture classifier model...
Downloading code formula model...
Downloading easyocr models...
Models downloaded into $HOME/.cache/docling/models.
```

Alternatively, models can be programmatically downloaded using `docling.utils.model_downloader.download_models()`.

**Step 2: Use the prefetched models**

```python
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import EasyOcrOptions, PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption

artifacts_path = "/local/path/to/models"

pipeline_options = PdfPipelineOptions(artifacts_path=artifacts_path)
doc_converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    }
)
```

Or using the CLI:

```
docling --artifacts-path="/local/path/to/models" FILE
```

Or using the `DOCLING_ARTIFACTS_PATH` environment variable:

```
export DOCLING_ARTIFACTS_PATH="/local/path/to/models"
python my_docling_script.py
```

#### Using remote services

The main purpose of Docling is to run local models which are not sharing any user data with remote services. Anyhow, there are valid use cases for processing part of the pipeline using remote services, for example invoking OCR engines from cloud vendors or the usage of hosted LLMs.

In Docling we decided to allow such models, but we require the user to explicitly opt-in in communicating with external services.

```python
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption

pipeline_options = PdfPipelineOptions(enable_remote_services=True)
doc_converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    }
)
```

When the value `enable_remote_services=True` is not set, the system will raise an exception `OperationNotAllowed()`.

_Note: This option is only related to the system sending user data to remote services. Control of pulling data (e.g. model weights) follows the logic described in [Model prefetching and offline usage](https://docling-project.github.io/docling/usage/#model-prefetching-and-offline-usage)._

##### List of remote model services

The options in this list require the explicit `enable_remote_services=True` when processing the documents.

-   `PictureDescriptionApiOptions`: Using vision models via API calls.

#### Adjust pipeline features

The example file [custom\_convert.py](https://docling-project.github.io/docling/examples/custom_convert/) contains multiple ways one can adjust the conversion pipeline and features.

You can control if table structure recognition should map the recognized structure back to PDF cells (default) or use text cells from the structure prediction itself. This can improve output quality if you find that multiple columns in extracted tables are erroneously merged into one.

```python
from docling.datamodel.base_models import InputFormat
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.pipeline_options import PdfPipelineOptions

pipeline_options = PdfPipelineOptions(do_table_structure=True)
pipeline_options.table_structure_options.do_cell_matching = False  # uses text cells predicted from table structure model

doc_converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    }
)
```

Since docling 1.16.0: You can control which TableFormer mode you want to use. Choose between `TableFormerMode.FAST` (faster but less accurate) and `TableFormerMode.ACCURATE` (default) to receive better quality with difficult table structures.

```python
from docling.datamodel.base_models import InputFormat
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode

pipeline_options = PdfPipelineOptions(do_table_structure=True)
pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE  # use more accurate TableFormer model

doc_converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    }
)
```

#### Impose limits on the document size

You can limit the file size and number of pages which should be allowed to process per document:

```python
from pathlib import Path
from docling.document_converter import DocumentConverter

source = "https://arxiv.org/pdf/2408.09869"
converter = DocumentConverter()
result = converter.convert(source, max_num_pages=100, max_file_size=20971520)
```

#### Convert from binary PDF streams

You can convert PDFs from a binary stream instead of from the filesystem as follows:

```python
from io import BytesIO
from docling.datamodel.base_models import DocumentStream
from docling.document_converter import DocumentConverter

buf = BytesIO(your_binary_stream)
source = DocumentStream(name="my_doc.pdf", stream=buf)
converter = DocumentConverter()
result = converter.convert(source)
```

#### Limit resource usage

You can limit the CPU threads used by Docling by setting the environment variable `OMP_NUM_THREADS` accordingly. The default setting is using 4 CPU threads.

#### Use specific backend converters

Note

This section discusses directly invoking a [backend](https://docling-project.github.io/docling/concepts/architecture/), i.e. using a low-level API. This should only be done when necessary. For most cases, using a `DocumentConverter` (high-level API) as discussed in the sections above should suffice¬†‚Äî¬†and is the recommended way.

By default, Docling will try to identify the document format to apply the appropriate conversion backend (see the list of [supported formats](https://docling-project.github.io/docling/usage/supported_formats/)). You can restrict the `DocumentConverter` to a set of allowed document formats, as shown in the [Multi-format conversion](https://docling-project.github.io/docling/examples/run_with_formats/) example. Alternatively, you can also use the specific backend that matches your document content. For instance, you can use `HTMLDocumentBackend` for HTML pages:

```python
import urllib.request
from io import BytesIO
from docling.backend.html_backend import HTMLDocumentBackend
from docling.datamodel.base_models import InputFormat
from docling.datamodel.document import InputDocument

url = "https://en.wikipedia.org/wiki/Duck"
text = urllib.request.urlopen(url).read()
in_doc = InputDocument(
    path_or_stream=BytesIO(text),
    format=InputFormat.HTML,
    backend=HTMLDocumentBackend,
    filename="duck.html",
)
backend = HTMLDocumentBackend(in_doc=in_doc, path_or_stream=BytesIO(text))
dl_doc = backend.convert()
print(dl_doc.export_to_markdown())
```

## Chunking

You can chunk a Docling document using a [chunker](https://docling-project.github.io/docling/concepts/chunking/), such as a `HybridChunker`, as shown below (for more details check out [this example](https://docling-project.github.io/docling/examples/hybrid_chunking/)):

```python
from docling.document_converter import DocumentConverter
from docling.chunking import HybridChunker

conv_res = DocumentConverter().convert("https://arxiv.org/pdf/2206.01062")
doc = conv_res.document

chunker = HybridChunker(tokenizer="BAAI/bge-small-en-v1.5")  # set tokenizer as needed
chunk_iter = chunker.chunk(doc)
```

An example chunk would look like this:

```python
print(list(chunk_iter)[11])
# {
#   "text": "In this paper, we present the DocLayNet dataset. [...]",
#   "meta": {
#     "doc_items": [{
#       "self_ref": "#/texts/28",
#       "label": "text",
#       "prov": [{
#         "page_no": 2,
#         "bbox": {"l": 53.29, "t": 287.14, "r": 295.56, "b": 212.37, ...},
#       }], ...,
#     }, ...],
#     "headings": ["1 INTRODUCTION"],
#   }
# }
```
    """
    
    # Î≤àÏó≠ ÏàòÌñâ
    translated = translator.translate(markdown_text)
    print("Î≤àÏó≠ Í≤∞Í≥º:")
    print(translated[:500])
    
    # ÌÜµÍ≥Ñ Ï†ïÎ≥¥
    stats = translator.get_translation_stats(markdown_text)
    print(f"Î≤àÏó≠ ÌÜµÍ≥Ñ: {stats}")
    

    # ÌååÏùº Î≤àÏó≠
    # input = "tests/Usage - Docling.md"
    # output = "tests/Usage - Docling_ko.md"
    # translator.translate_file(input, output)
